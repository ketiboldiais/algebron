import { LA1, LA2, LA3, LA4, LA5 } from "@/components/Fig";
import MdxLayout from "@/components/mdx-layout";

# Linear Algebra

<TOC>

- [Vectors](#vectors)
- [Real Vectors](#real-vectors)
  - [Scalar Multiplication](#scalar-multiplication)
- [Systems of Linear Equations](#systems-of-linear-equations)
- [Matrices](#matrices)
  - [Matrix Addition](#matrix-addition)
  - [Matrix Multiplication](#matrix-multiplication)
    - [Hadamard Product](#hadamard-product)
    - [Scalar Multiplication](#scalar-multiplication-1)
    - [Row-Column Dot Product](#row-column-dot-product)

</TOC>

## Vectors
A vector is a mathematical object that can be added to another vector or multiplied by a scalar to produce another vector. This general definition yields numerous examples:

1. **Geometric Vectors.** We can think of a vector as a geometric object with a magnitude (a line of given length) and a direction (an arrowhead). For example, consider two vectors ${a = [3,3]}$ and ${b = [4,1].}$ The sum of these vectors is ${a + b = [3 + 4, 3 + 1],}$ which reduces to ${[7,4].}$ Geometrically:

   <LA1 />

   Similarly, we can _scale_ the geometric vector, which will still yield a vector, longer, shorter, or just the same.

   <LA3 />

2. **Polynomials.** Polynomials may also be interpreted as vectors. Consider the two polynomials:

   $$
       3x^2 + 4x - 1 \\
       4x^2 - 2x + 2
   $$

   We can represent these expressions as the vectors ${[3,4,-1]}$ and ${[4,-2,2],}$ where the polynomials' coefficients comprise the vector components. Adding these two vectors corresponds to adding the two polynomials, the sum of which is itself a polynomial (i.e., a vector). Thus,

   $$
       [3,4,-1] + [4,-2,2] = [7,2,1]
   $$

   maps to the polynomial

   $$
       7x + 2x + 1.
   $$

   As an aside, recall that if we plot the two polynomials, the point where they intersect is the solution to the polynomial system. We will revisit this curiousity in later sections.

   <LA2 />

3. **Audio Signals.** Audio signals can also be interpreted as vectors. We can add audio signals together to produce another audio signal, and we can also scale them to produce yet another.

   <LA4 />

4. **Tuples.** More abstractly, elements of ${\reals^n}$ (tuples, or arrays, of ${n}$ real numbers) are vectors.

## Real Vectors

For now, we'll focus only on vectors in ${\reals^n.}$ That is, vectors with ${n}$ components, all of which are real numbers.

> **Definition: Real Zero Vector**. The zero vector in ${\reals^n}$ is the vector ${\bold{0}^n,}$ whose components are all zero.
>
> $$
>    \bold{0}^n = \begin{bmatrix}
>    0_1 \\
>    0_2 \\
>    \vdots \\
>    0_n
>     \end{bmatrix}.
> $$

The zero vector is special for a few reasons:

1. Given a vector ${\bold{v} \in \reals^n,}$ adding the zero vector ${\bold{0}^n}$ will always return ${\bold{v}.}$ Because of this result, we call the zero vector ${\bold{0}^n}$ the **additive identity** of ${\reals^n.}$
2. The zero vector is the only vector with a magnitude of zero.
3. The zero vector is the only vector with no direction.

### Scalar Multiplication

Scalar multiplication is straightforward: We multiply each component of the vector by the given scalar ${k.}$ Geometrically (in ${\reals^2}$ or ${\reals^3}$), if ${k \gt 0,}$ this has the effect of making the vector shorter or longer.

> **Definition: Scalar Multiplication.** Let ${\bold{v}}$ be a vector in ${\reals^n,}$ and let ${k \in \reals}$ be a scalar. Then:
>
> $$
>     k \bold{v} = \begin{bmatrix}
>     k\bold{v}_1 \\
>     k\bold{v}_2 \\
>     \vdots \\
>     k\bold{v}_n
>     \end{bmatrix}.
> $$

**_example_**.

$$
    3 \begin{bmatrix}
    5 \\
    8 \\
    7
    \end{bmatrix}
    =
    \begin{bmatrix}
    3 \times 5 \\
    3 \times 8 \\
    3 \times 7
    \end{bmatrix}
    =
    \begin{bmatrix}
    15 \\
    24 \\
    21
    \end{bmatrix}.
$$

Since ${k}$ can be any real number, scalar multiplication's definition implies that we can negate a vector and perform element-wise division.

> **Definition: Vector Negation.** Given a vector ${\bold{v},}$ the negation of ${\bold{v},}$ denoted ${-\bold{v},}$ is:
>
> $$
>     -\bold{v} = \begin{bmatrix}
>     -1 \times \bold{v}_1 \\
>     -1 \times \bold{v}_2 \\
>     \vdots \\
>     -1 \times \bold{v}_n
>     \end{bmatrix}
>     =
>     \begin{bmatrix}
>     -\bold{v}_1 \\
>     -\bold{v}_2 \\
>     \vdots \\
>     -\bold{v}_n
>     \end{bmatrix}
>     .
> $$

Geometrically, negating a vector ${\bold{v}}$ "switches" its direction.

## Systems of Linear Equations

Many problems in mathematics can be reduced to systems of linear equations. Linear algebra provides us the tools for solving these systems.

> **Definition.** The system
>
> $$
> \begin{aligned}
>     a_{1,1}x_1 &+ &\ldots &+ a_{1,n}x_n = b_1 \\
>     &\vdots & &\vdots  \\
>     a_{m,1}x_1 &+ &\ldots &+ a_{m,n}x_n = b_n
> \end{aligned}
> $$
>
> is called a **system of linear equations**, where ${a_{i,j}, b_{i} \in \reals}$ and each ${x_i}$ is a real number called an _unknown_ of the system.

Given a system of linear equations ${S,}$ each ${n}$-tuple ${(x_1, \ldots, x_n) \in \reals^n}$ that satisfies ${S}$ is called a _solution_ of ${S.}$ In a system with two unknowns ${x_1}$ and ${x_2,}$ each linear equation defines a line on the ${x_1}$-${x_2}$ plane. E.g.,

$$
    4x_1 + 4x_2 = 5 \\
    2x_1 - 4x_2 = 1
$$

is such a system. Note that we chose the variables ${x_1}$ and ${x_2}$ arbitrarily. We could've just used ${x}$ and ${y:}$

$$
    4x + 4y = 5 \\
    2x - 4y = 1.
$$

Notice that these are simple linear equations. Solving for ${y:}$

$$
    y = -x + \dfrac{5}{4} \\[1em]
    y = \dfrac{x}{2} - \dfrac{1}{4}.
$$

The point where these lines intersect is a solution to the system. In this case, the point ${(1, \frac{1}{4}).}$ Thus, the system above has the **solution space** of ${\lbrace (1, \frac{1}{4}) \rbrace.}$

<LA5 />

We can represent a system of linear equations with matrices. We place the coefficients ${a_{i,j}}$ into vectors,

$$
    \begin{bmatrix}
    a_{1,1} \\ \vdots \\ a_{m,1}
    \end{bmatrix}
    x_1
    +
    \begin{bmatrix}
    a_{1,2} \\ \vdots \\ a_{m,2}
    \end{bmatrix}
    x_2
    +
    \ldots
    +
    \begin{bmatrix}
    a_{1,n} \\ \vdots \\ a_{m,n}
    \end{bmatrix}
    x_n
    =
    \begin{bmatrix}
    b_1 \\ \vdots \\ b_m
    \end{bmatrix},
$$

then place the vectors into matrices

$$
    \begin{aligned}
    \begin{bmatrix}
    a_{1,1} &\ldots &a_{1,n} \\
    \vdots   &~     \vdots &\vdots \\
    a_{m,1} &\ldots &a_{m,n}
    \end{bmatrix}
    \end{aligned}

    \begin{bmatrix}
    x_1 \\ \vdots \\ x_n
    \end{bmatrix}
    =
    \begin{bmatrix}
    b_1 \\ \vdots \\ b_m
    \end{bmatrix}.
$$

## Matrices

A matrix can be thought of as an array of arrays. They allow us to compactly represent a system of linear equations.

> **Definition.** Let ${m,n \in \N.}$ A real-valued ${(m,n)}$ matrix ${\bold{A}}$
> is an ${m \cdot n}$ tuple of elements ${a_{i,j},}$ ${i = 1, \ldots, m,}$ ${j =
> 1, \ldots, n.}$ This tuple is ordered as a rectangle of ${m}$ rows and ${n}$
> columns:
>
> $$
>     \bold{A} =
>     \begin{bmatrix}
>     a_{1,1} & a_{1,2} & \ldots & a_{1,n} \\
>     a_{2,1} & a_{2,2} & \ldots & a_{2,n} \\
>     \vdots  & \vdots  & \vdots & \vdots \\
>     a_{m,1} & a_{m,2} & \ldots & a_{m,n}
>     \end{bmatrix},
>     ~~ a_{i,j} \in \R.
> $$
>
> We call ${i}$ the _row index_, and ${j}$ the _column index_.

Some additional terminology: A ${(1,n)}$ matrix is called a **row vector**, and a ${(m,1)}$ matrix is called a **column vector**.

**_example_**. In programming, we can represent a row vector with a simple array:

```ts
let R = [1, 2, 3, 4];
```

**_example_**. A matrix would be an array of arrays:

```ts
let M = [
  [1, 2, 3],
  [4, 5, 6],
  [7, 8, 9],
];
```

**_example_**. A column vector would be an array of one-element arrays:

```ts
let C = [[1], [2], [3]];
```

### Matrix Addition

To add two matrices, we add each element of one matrix to the corresponding element (the element of the same indices) in the other matrix (element-wise sum).

**Definition.** Let ${\bold{A} \in \R^{m \times n},}$ and let ${\bold{B} \in \R^{m \times n}.}$ Then the sum ${\bold{A} + \bold{B}}$ is defined as:

$$
    \bold{A} + \bold{B} :=
    \begin{bmatrix}
    a_{1,1} + b_{1,1} & \ldots & a_{1,n} + b_{1,n} \\
    \vdots & \vdots & \vdots \\
    a_{m,1} + b_{m,1} & \ldots & a_{m,n} + b_{m,n}
    \end{bmatrix}
    \in
    \reals^{m \times n}.
$$

**_example_**.

$$
\begin{aligned}
    \begin{bmatrix}
    3 & 0 & 3 \\
    5 & 7 & 4 \\
    2 & 1 & 8
    \end{bmatrix}
    +
    \begin{bmatrix}
    1 & 2 & 5 \\
    4 & 1 & 4 \\
    3 & 0 & 1
    \end{bmatrix}
    &=
    \begin{bmatrix}
    3 + 1 & 0 + 2 & 3 + 5 \\
    5 + 4 & 7 + 1 & 4 + 4 \\
    2 + 3 & 1 + 0 & 8 + 1
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
    4 & 2 & 8 \\
    9 & 8 & 8 \\
    5 & 1 & 9
    \end{bmatrix}
\end{aligned}.
$$

### Matrix Multiplication

There are three forms of matrix multiplication: **scalar multiplication**, the **Hadamard product**, and the **row-column dot product**.

#### Hadamard Product

The Hadamard product is simply element-wise multiplication.

**Definition.** Let ${\bold{A} \in \R^{m \times n},}$ and let ${\bold{B} \in \R^{m \times n}.}$ Then the Hadamard product ${\bold{A} \odot \bold{B}}$ is defined as:

$$
    \bold{A} \odot \bold{B} :=
    \begin{bmatrix}
    a_{1,1}b_{1,1} & \ldots & a_{1,n}b_{1,n} \\
    \vdots & \vdots & \vdots \\
    a_{m,1}b_{m,1} & \ldots & a_{m,n}b_{m,n}
    \end{bmatrix}
    \in
    \reals^{m \times n}.
$$

Like the matrix sum, the Hadamard product is defined only if ${\bold{A}}$ and ${\bold{B}}$ have the same dimensions.

<aside>
  The Hadamard product is attributed to, and named after, the French
  mathematician Jacques Hadamard (1865 - 1963). Hadamard was a leading
  contributor in the fields of number theory, complex analysis, differential
  geometry, and partial differential equations. Hadamard's second cousin, Lucie,
  was the wife of Alfred Dreyfus of the Dreyfus affair.
</aside>

**_example_**.

$$
\begin{aligned}
    \begin{bmatrix}
    2 & 7 & 5 \\
    9 & 4 & 3 \\
    1 & 3 & 8
    \end{bmatrix}
    +
    \begin{bmatrix}
    7 & 3 & 5 \\
    9 & 8 & 4 \\
    1 & 6 & 3
    \end{bmatrix}
    &=
    \begin{bmatrix}
    2 \times 7 & 7 \times 3 & 5 \times 5 \\
    9 \times 9 & 4 \times 8 & 3 \times 4 \\
    1 \times 1 & 3 \times 6 & 8 \times 3
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
    14 & 21 & 25 \\
    81 & 32 & 12 \\
    1 & 18 & 24
    \end{bmatrix}
\end{aligned}.
$$

#### Scalar Multiplication

When we multiply a scalar to a matrix, we multiply each element of the matrix with the scalar.

#### Row-Column Dot Product

With matrix sums, we simply perform element-wise addition. Matrix multiplication, however, looks a little different:

> **Definition.** Given matrices ${\bold{A} \in \reals^{m \times n}}$ and${\bold{B} \in \reals^{n \times k},}$ the elements ${c_{i,j}}$ of the product ${\bold{C} = \bold{A}\bold{B} \in \reals^{m \times k}}$ are computed as:
>
> $$
>     c_{i,j} = \sum_{\ell = 1}^{n} a_{i,\ell} b_{i,\ell,} \\[1em]
>     i = 1, \ldots, m \\
>     j = 1, \ldots, k.
> $$

That is, to compute the element ${c_{i,j},}$ multiply the ${i^{\text{th}}}$ row of ${\bold{A}}$ with the ${j^{\text{th}}}$ column of ${\bold{B}}$ and sum them.

**_example_**. To compute the row-column dot product of

$$
    \underbrace{
    \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6
    \end{bmatrix}}_{\bold{A}}
    \cdot
    \underbrace{
    \begin{bmatrix}
    7 & 8 \\
    9 & 10 \\
    11 & 12
    \end{bmatrix}}_{\bold{B}},
$$

we start by computing the dot product of the first row of ${A}$ and the first column of ${B:}$

$$
\begin{aligned}
    [1,2,3] \cdot [7,9,11] &= (1 \times 7) + (2 \times 9) + (3 \times 11) \\
    &= 58.
\end{aligned}
$$

This gives us the first row, first column entry:

$$
    \begin{bmatrix}
    58 & ~ \\
    ~ & ~
    \end{bmatrix}.
$$

Next we compute the first row, second column dot product:

$$
\begin{aligned}
    [1,2,3] \cdot [8,10,12] &= (1 \times 8) + (2 \times 10) + (3 \times 12) \\
    &= 64.
\end{aligned}
$$

This gives us the first row, second column entry:

$$
    \begin{bmatrix}
    58 & 64 \\
    ~ & ~
    \end{bmatrix}.
$$

Now the second row, first column:

$$
\begin{aligned}
    [4,5,6] \cdot [7,9,11] &= (4 \times 7) + (5 \times 9) + (6 \times 11) \\
    &= 139.
\end{aligned}
$$

Thus, we now have:

$$
\begin{bmatrix}
58 & 64 \\
139 & ~
\end{bmatrix}.
$$

Finally, compute the second row, second column:

$$
\begin{aligned}
[4,5,6] \cdot [8,10,12] &= (4 \times 8) + (5 \times 10) + (6 \times 12) \\
&= 154.
\end{aligned}
$$

The result:

$$
\begin{bmatrix}
58 & 64 \\
139 & 154
\end{bmatrix}.
$$

Notice: Given a matrix ${\bold{A}}$ and a matrix ${\bold{B},}$ the row-column product ${\bold{A} \cdot \bold{B}}$ is defined only if the number of columns of ${\bold{A}}$ is equal to the number of rows of ${\bold{B}.}$ In general, given an ${m \times n}$ matrix ${\bold{A}}$ and an ${n \times k}$ matrix ${\bold{B},}$ ${\bold{A}\cdot\bold{B}}$ is defined only if the ${n}$s are the same. The result ${\bold{A} \cdot \bold{B}}$ is an ${m \times k}$ matrix.

export default function MDXPage({ children }) {
    return <MdxLayout>{children}</MdxLayout>

}
